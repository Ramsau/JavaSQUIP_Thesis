\documentclass[11pt,
  titlepage=false,
  %parskip=half,      % enable if you want paragraphs separated by vertical spacing instead of indents
]{scrreprt}

% Style settings
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\addtokomafont{disposition}{\rmfamily}

\usepackage[bsc,             % for Bachelor's Thesis titlepage
%project,        % for Master's Project titlepage
    claim,iaik]{iaikthesis}

\makeatletter\patchcmd{\scr@startchapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}\makeatother

% Useful packages for complex content:
\usepackage{amsmath,amsfonts,amssymb} % typesetting math
%\usepackage{siunitx}                 % typesetting SI-units and formatted numbers
%\usepackage{listings}                % typesetting source code
\usepackage{booktabs,multirow}        % utils for complex/beautiful tables
%\usepackage{subcaption}              % placing multiple subfigures in a figure
%\usepackage{graphicx}                % including external image files
%\usepackage{tikz}                    % drawing figures within LaTeX

% Bibliography, referencing, and indexing
\usepackage{csquotes}                 % typesetting \enquote{text in quotes} correctly
\usepackage[backend=biber,
            style=alphabetic,
            minalphanames=3, maxalphanames=4,
            maxbibnames=20]{biblatex} % to generate the bibliography
\addbibresource{report.bib}           % name of the bib-file

% Useful utils:
%\usepackage{todonotes}               % add ToDo markers (\todo{...}, \todo[inline]{...})
\usepackage[hidelinks]{hyperref}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{listings}      % clickable links (but hide color frames around links)
%\usepackage{cleveref}                % named references (\Cref{chap:introduction}, ...)

% Your own macros:
%\newcommand{\mynewmacro}[1]{my content with one input parameter: #1}


\usepackage{color}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  ndkeywords={class, export, boolean, throw, implements, import, this},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  sensitive=true
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}

\begin{document}

%--- FRONT MATTER --------------------------------------------------------------

%--- INSERT INFORMATION FOR TITLEPAGE ------------------------------------------

% Your name + previous academic degrees:
\thesisauthor{Christoph Royer}

% Title of your thesis:
\thesistitle{JavaSQUIP\\An application of a scheduler queue-based covert channel to JavaScript}

% Date of completion:
\thesisdate{Month Year}

% Supervisor:
%\supervisortitle{Supervisors}
\supervisor{%
    Advisor: Stefan Gast\\
    Supervisor: Daniel Gruss
    \smallskip

    Institute of Applied Information Processing and Communications\\
    Graz University of Technology
}

% Name of your degree programme according to your curriculum
\curriculum{Computer Science}

\printthesistitle

\chapter*{\centering\Large Abstract}
\label{ch:abstract}
The implementation of out-of-order execution has brought a big performance benefit to current CPUs.
But with this benefit also come security concerns, since attackers can exploit the timing variations which are bound to occur in an out-of-order pipeline.
Attacks which target only one execution unit through a separate scheduler queue -- as seen in the Apple M1 and AMD Zen 2 and Zen 3 microarchitectures -- have proven even more powerful than port contention on single-scheduler systems.
As with any attack, porting this side channel to the web would greatly increase its reach and number of victims.

In this paper, we present the JavaSQUIP attack, which is a port of SQUIP~\cite{squip} to JavaScript.
We look into the security measures used in common browsers to prevent timing attacks, and show how we worked around them successfully.

Our covert channel can provide communication across separate browser instances at a speed of 1000 bits/s,
which is faster than any current covert channel of this type.
At this speed, we were able to achieve a transmission accuracy of $99.2\%$ and $99.3\%$ on Zen 3 and Zen 4 respectively.

JavaSQUIP makes it clear that current browsers have not yet been able to adapt to be secure in light of the ever increasing complexity of modern CPUs.
Our findings suggest that making these attacks impossible may not even be feasible because the drawbacks in performance and features would be too large.
\paragraph{Keywords:}
Covert channel $\cdot$
Scheduler queue contention $\cdot$
JavaScript $\cdot$
AMD Zen

\clearpage

%--- INTRODUCTION --------------------------------------------------------------

\chapter{Introduction}
\label{ch:introduction}

Nowadays, CPU manufacturers are in a steady competition against each other to make their product faster and faster.
One of the ways this is achieved is \textit{out-of-order execution}, where instructions are not executed in the order given by the binary,
but rather as soon as their respective dependencies are ready and a suitable execution unit is free.
Although this toes the line of breaking the hardware-software contract, the CPU ensures that out-of-order execution remains transparent to the user.
To better balance usage of its execution units, modern CPUs often also include \textit{Simultaneous Multithreading} (SMT).
Here, one physical core is split up into two logical cores, which share physical infrastructure like the schedulers and execution units.

Though out-of-order execution and SMT are designed to be transparent to the user, attacks like
SMoTherSpectre~\cite{Bhattacharyya2019} and PortSmash~\cite{Aldaya2019port} show that the combination of the two forms a viable attack surface.
By measuring timing variations in out-of-order execution, an attacker can gain insight into sensitive information.
The two attacks achieve this is through \textit{port contention}.
%Because the instructions are reordered on the fly, the CPUs scheduler needs to distribute incoming instructions across the ports for its several specialized execution units.
If the attacker is co-located with the victim on the same core, they can measure delays when issuing instructions to a shared execution unit.
In this way, the attacker can gain knowledge on the victim's execution based on whether instructions are delayed by the scheduler or not.

SQUIP~\cite{squip} focuses on CPUs with multiple specialized scheduler queues to implement \textit{scheduler queue contention}.
Because each scheduler deals with fewer execution units, the attacker can push one of the comparatively small scheduler queues close to its capacity.
If the victim also adds to the queue, the pipeline stalls, which results in a big spike in execution time
This makes SQUIP~\cite{squip} a fast and reliable covert channel on many modern CPU architectures such as the AMD Zen 2 and Zen 3.

Since implementations of port contention-based side channels in a browser setting already exist~\cite{Rokicki2022webport}, we ask the following research question:

\textit{Can per-execution-unit scheduler queue contention be exploited in a browser setting? Are there significant benefits compared to single scheduler port contention-based approaches?}

In this paper, we present JavaSQUIP, a browser-based covert channel using the SQUIP~\cite{squip} attack.
We show that it is reliable and faster than approaches based on CPU architectures with a single scheduler queue.
We develop a framework for transmitting data between separate instances of a browser at speeds of up to 1000 bit/s.

\paragraph{Outline.}
In Section~\ref{ch:background}, we provide relevant background information on pipelines, schedulers and scheduler queue contention.
We also observe several challenges that come with trying to implement a timing attack in a browser.
Section~\ref{ch:implementation} shows how we dealt with these challenges.
We go into more detail on the mode and results of our evaluation of JavaSQUIP in Section~\ref{ch:evaluation}.
Section~\ref{ch:conclusion} concludes by comparing JavaSQUIP's performance to similar attacks and outlining potential use cases.


%--- BACKGROUND ----------------------------------------------------------------

\chapter{Background}
\label{ch:background}

In this chapter, we go into detail on CPU scheduling and how it has been exploited in the past.
Afterwards, we take a closer look into the browser features needed to implement these exploits in JavaScript.

\section{Simultaneous Multithreading}
\label{sec:smt}

As will be described in more detail in Chapter~\ref{sec:cpuschedulers},
the cores of a modern CPU are subdivided into multiple specialized execution units.
If only one thread would be executed on such a core,
most of these execution units would stay unused while an instruction is executed.

With \textit{Simultaneous Multithreading} (SMT), one physical cores is split into two logical cores.
This means that one core executes two threads simultaneously, distributing the workload across its execution units more effectively.
This provides a potential speed-up of up to $100\%$ with relatively little additional architecture needed~\cite{tullsen1995simultaneous}.

The two threads that are being executed on the same core are called \textit{co-located}.
Usually, these threads should be unaffected by each other; no data are exchanged between two co-located threads.
However, the sharing of common architecture, particularly the execution units,
makes possible several different side-channels -- most of them timing attacks.
A selection of these will be explained in the following chapter.

\section{CPU Scheduling}
\label{sec:cpuschedulers}
For performance reasons, modern CPUs use efficiency features at almost every stage of an instruction's execution.
In the following, we will list some of these features for each of the stages along with the way they are exploitable.
Each of the attacks outlined in this chapter relies on the attacker being co-located with the victim,
i.e. being executed on the same physical core.

\textbf{Fetch.}
Before an instruction can be executed, it needs to be loaded from cache or memory.
As branches can make loading unpredictable and loading the wrong instruction is expensive, a \textit{branch prediction unit} tries to guess which path is more likely to be loaded.
Taking this one step further, the predicted path may also be executed before the condition for the branch has been computed.
This is called \textit{speculative execution}\cite{AMD2020OptimizationEPYC7002}, and it can be exploited, as was shown in the Spectre~\cite{spKocherHFGGHHLM019} attack.

\textbf{Decode.}
Modern CISC CPUs divide normal instructions into their parts when possible;
these parts are called \textit{micro-ops} (\textmu ops)\cite{AMD2020OptimizationEPYC7002}.
For example, an addition with memory operands may be split up into three \textmu ops: one for loading the operands, one for the addition, and one for storing the result.
These \textmu ops can be distributed across specialized execution units, enabling more parallelisation and load balancing, and thus an increase in performance.

\textbf{Schedule/Execute.}
Once created, the \textmu ops are queued to be executed as soon as their dependencies are met -- sometimes out of the order of their arrival.
Depending on the CPU microarchitecture, there can be one general scheduler queue for all execution units, or multiple.
While Intel uses one scheduler queue for all its execution units~\cite{Intel_opt},
architectures like AMD's Zen2~\cite{AMD2020OptimizationEPYC7002} and Zen3~\cite{AMD2020OptimizationEPYC7003} use several specialized scheduler queues.
With multiple scheduler queues, developers can specialize queues for a specific \textmu op or class of \textmu ops.
Figures \ref{fig:amdzen2} and \ref{fig:amdzen3} illustrate the concept.
Splitting the scheduler queue reduces complexity and enables more efficient power usage.
The downside is -- as we will see in the following paragraphs -- that these smaller queues can be exploited for side channel attacks.
An attacker that is co-located to a victim has two vectors for a timing attack: \textit{port contention} and \textit{scheduler queue contention}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Zen2 arch}

    \caption{The scheduler layout in AMD Zen2 (c.f.~\cite{AMD2020OptimizationEPYC7002})}
    \label{fig:amdzen2}

    \includegraphics[width=0.5\textwidth]{figures/Zen3 arch}

    \caption{The scheduler layout in AMD Zen3 (c.f.~\cite{AMD2020OptimizationEPYC7003})}
    \label{fig:amdzen3}
\end{figure}

SMoTherSpectre~\cite{Bhattacharyya2019} and PortSmash~\cite{Aldaya2019port} implement port contention.
They use the fact that only one of the two threads in an SMT-enabled core can use an execution unit in a single cycle.
The attacker can thus find out whether the victim is using a port by measuring the execution time of an instruction on the same port.

SQUIP~\cite{squip} on the other hand implements scheduler queue contention.
In scheduler queue contention, the attacker fills up the scheduler queue to the point that it is almost at its capacity limit;
any activity of the victim that involves this scheduler queue will overflow the queue, leading to a queue stall.
A queue stall occurs when more \textmu ops of a particular type are created than what the correlating scheduler queue can hold.
Because the CPU cannot drop any \textmu ops, it has to stall the loading of new instructions entirely until the scheduler queue is free again.
The attacker can measure this easily because it entails a significant spike in execution time --
not only for instructions in the affected queue, but on the whole core.
\ref{fig:amdzen2sqc} shows how one instruction stream can stall the other just by filling up the relatively small scheduler queue.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Zen2 normal operation}

    \caption{An AMD Zen2 processor in normal operation: the two instruction streams do not impede each other}
    \label{fig:amdzen2normaloperation}

    \includegraphics[width=0.75\textwidth]{figures/Zen2 sqc}

    \caption{An AMD Zen2 processor exhibiting scheduler queue contention: a stall on the Scheduler queue for ALU1 stalls both instruction streams}
    \label{fig:amdzen2sqc}
\end{figure}

SQUIP~\cite{squip} is limited in its applicable CPU architectures because it relies on the split-scheduler architecture of AMD Zen2 and Zen3 CPUs to distinguish between \textmu ops.
However its advantage is a higher transmission rate compared to port contention.
The bigger spikes in execution times of a stalled scheduler queue (whole CPU) compared to port contention (one execution unit) make measurement easier.
Also, SQUIP~\cite{squip} can measure execution times on different execution unit than the one producing the stalls.
All of this makes SQUIP~\cite{squip} a faster and more reliable side channel than port-contention based side channels.
Because of this, JavaSQUIP aims to port this vulnerability to a browser environment.


\textbf{Retire.}
After all the \textmu ops of an instruction have completed, the \textit{retire control unit} (RCU) reassembles the results.
It commits the finished instructions and schedules dependent instructions -- the dissection into \textmu ops is transparent to the user.


\section{Browser security measures}
\label{sec:browsersecurity}
Since the advent of many timing side- and covert channels~\cite{noack2018exploiting, Rokicki2022webport, gruss2016rowhammer},
browser developers have implemented many measures to mitigate the threat~\cite{shusterman2021prime, performancenow, performancenowchrome}.
This has always meant striking a balance between security and efficiency.
Because of this, many potential attack surfaces cannot be eliminated without unreasonable losses in performance and functionality,
though some measures do slow down side- and covert channels significantly.

In the following we expand on security measures regarding multithreading, accurate timing in browsers, and low-level operations.
All three of these are needed to mount the SQUIP attack.


\subsection{Timing functions}\label{subsec:timingjs}
There are two contenders for timing measurements in JavaScript: \texttt{Date.now()}~\cite{datenow} and \texttt{performance.now()}~\cite{performancenow}.

The \texttt{Date.now()}~\cite{datenow} function gives a synchronized Unix epoch timestamp with millisecond accuracy.
The Unix epoch time is essentially the time elapsed since January 1, 1970 and is the same for all processes on a computer.
This makes \texttt{Date.now()} a cross-instance synchronized timestamp.

\texttt{performance.now()}~\cite{performancenow} measures the time elapsed since the \textit{time origin} of the current context.
This means that \texttt{performance.now()} gives different values between browser instances, and even between a main thread and its worker threads.
Timing information from \texttt{performance.now()} can still be synchronized;
The \texttt{performance.timeOrigin}~\cite{performanceTimeOrigin} property gives an accurate timestamp with 1 ms accuraccy.

The accuracy of \texttt{performance.now()} has been heavily restricted by browsers to counteract the threat of timing attacks.
For example, in Firefox it has an accuracy of 1 ms, and in Chrome it measures with an accuracy of 0.1 ms~\cite{performancenow, performancenowchrome}.

Thus both \texttt{Date.now()} and \texttt{performance.now()} give a synchronized timestamp with 1 ms accuracy, making them viable options for synchronization.

\subsection{Multithreading}\label{subsec:multithreading}
Though JavaScript does not support low-level control over threading, the \texttt{Web Workers} API~\cite{webworkers} allows the user to create workers running in a separate thread.
An instantiated worker runs a predefined script.
The main thread can interface with the worker via calls to the \texttt{Worker.postMessage()} function and via a \texttt{SharedArrayBuffer}~\cite{sharedarraybuffer}.
There is no support for selecting a core on which a worker is run on, presenting a challenge to co-locate with the sender.

Creating a \texttt{SharedArrayBuffer} was restricted in two ways: the browser window needs to be SSL-encrypted, and it needs to restrict some Cross-Origin requests.
Nevertheless, this is not a problem for this attack since the site provides \texttt{https} and all code comes from a single origin.


\subsection{Low-level operations in JavaScript}\label{subsec:lowleveljs}
JavaScript handles numbers as floats by default.
This means that a simple invocation of a multiplication like \texttt{a * b;} in JavaScript will not result in an \texttt{imul} instruction on the CPU.
Rather, \texttt{Math.imul(a, b)}~\cite{mathimul} gives the user a lower-level command:
It interprets the parameters as 32bit integers and multiplies them as such, resulting in an \texttt{imul} instruction on the CPU.


%--- IMPLEMENTATION ----------------------------------------------------------------
\chapter{Implementing JavaSQUIP}
\label{ch:implementation}
As described in Section~\ref{sec:browsersecurity}, there are some challenges to overcome when implementing a timing attack in a browser.
This chapter explains how we worked around these challenges to implement the JavaSQUIP covert channel.

\section{Getting accurate timing}
\label{sec:accurate-timing}
JavaSQUIP needs two kinds of timing in order to work properly:
\begin{itemize}
    \item a cross-browser syncronized clock to determine the start of bit transmissions:\\
    resolution of ${f_{\text{transmission}}}^{-1} \approx$ 1ms
    \item a fine-grained measurement to measure port contention:\\
    resolution of 100 CPU cycles $\approx$ 25ns
\end{itemize}

For the synchronized clock, we can use \texttt{Date.now()}~\cite{datenow}, which has a 1ms accuracy.
This allows us to partition the duration of the transmission into 1ms timeslices.
During one timeslice, a single bit can be transmitted by the existence or absence of port contention.
The alignment of bytes is solved by starting the transmission on a multiple of 8 timeslices since the start of the epoch (see \ref{subsec:timingjs}).
Synchronizing with \texttt{Date.now()}~\cite{datenow} only works for transmissions of up to 1000 bits/s,
since there is no more precise timestamp in JavaScript which is synchronized across browser instances.

%To get around this and realize a faster transmission than 1000 bits/s, we propose implementing the Manchester Code~\cite{manchesterEncoding}.
%It is self-synchronizing, which eliminates the need for an external source of synchronization.
%The actual implementation is not within the scope of this paper.

The receiver also needs a way to measure the small variations in timing whenever a scheduler queue gets blocked.
Since no timing function in JavaScript gives a better accuracy than 0.1ms, we need to provide our own timing.

We solved this with a counting worker, which continually increases one variable in the \texttt{SharedArrayBuffer}.
As explained in section \ref{subsec:multithreading}, the other receiver workers can then access this counter and use it to measure execution times with adequate accuracy.

\section{Co-locating with the sender}
\label{sec:co-location}
For co-locating the receiver with the sender, the receiver creates as many receiving workers as the number of virtual cores in the machine, minus two:
one for the counting thread (see \ref{sec:accurate-timing}), and one for the sending thread.
For the 8-core CPUs we used in our experiments (TODOCPU), this comes to $8 \times 2 - 2 = 14$ receiver workers.
Since every thread does continuous work, each thread will most likely be assigned to a unique virtual core.

To receive the message, one of the worker threads needs to be co-located with the sending thread.
Assuming a uniform distribution for the threads, we can calculate the theoretical co-location probability.
The only way that the sender is not co-located with a receiver is that the sender is co-located with the counting thread,
which is a $\frac{1}{15}$ chance.
Thus, the theoretical co-location probability is $\frac{14}{15} \approx 93.3\%$

% TODO figuring out the co-located worker

\section{Targeting a scheduler queue}
JavaSQUIP targets AMD Zen, Zen2, and Zen3;
these architectures support multiplication only on ALU1~\cite{AMD2020OptimizationEPYC7003}.
When sending a 1, JavaSQUIP makes multiple codependent calls to \texttt{Math.imul()}, which translate to the \texttt{imul} instruction.
This fills up the scheduler queue of ALU1, eventually causing a queue stall.
The codependence of the \texttt{imul} operations ensures that they have to be executed one after the other, in order.
It is ensured simply by using the result of one operation as the input for the next.

The following code snippet shows two functions of the sending process.
The functions are for sending a zero or a one respectively, in a given time frame.
The parameter \texttt{until} gives the end of the timeslice to send a bit.
\begin{lstlisting}[language=JavaScript]
function send_zero(until) {
  until = +until;

  while(+Date.now() < +until) {
    // simply busy waiting
  }
}

function send_one(until) {
  until = +until;

  let i = dummy[0]|0;
  let j = dummy[1]|0;
  while(+Date.now() < until) {
    i = (((i|0) + 100069) % 100103) | 0;
    j = (((j|0) + 997) % 13) | 0;

    // block scheduler queue with 100 imul instructions
    i = Math.imul(i|0, j|0) | 0;
    i = Math.imul(i|0, j|0) | 0;
    // repeats 100 times
    i = Math.imul(i|0, j|0) | 0;
    i = Math.imul(i|0, j|0) | 0;
  }

  // use the result so the calculation is not optimized away
  dummy[1] ^= i|0;
}
\end{lstlisting}

\section{Detecting scheduler queue contention}
\label{sec:detect-sqc}
The receiving process needs to detect whether the sender has sent a \texttt{1} or a \texttt{0}.
This is equivalent to detecting scheduler queue contention.
This was implemented by issuing several \texttt{imul} instructions,
and measuring the delay with the help of a separate counting thread.

The following code snippet shows how the receiver issues 10 \texttt{sqrt} and 20 \texttt{imul} instructions,
retreiving the counter value before and after.
If the retire control unit was not blocked by an overflowing scheduler queue,
the instructions can be reordered so both reads of the counting value are right after one another.
This results in the measured time delay being zero, which means no scheduler queue contention took place.
If the two values are not equal, we can deduce that scheduler queue contention is in progress.

\begin{lstlisting}[language=JavaScript]
    function squip(start_value, factor) {
  start_value = +start_value;
  factor = factor|0;

  let dummy = +start_value;
  let dummy2 = start_value|0;

  /*
  for(let i=0; i<1000; ++i)     //delay loop to drain the scheduler queue
    dummy2 ^= ((i|0)+997)%17;
  */

  let start_time = Atomics.load(timer, 0)|0;

  dummy = +Math.sqrt(+dummy);
  dummy = +Math.sqrt(+dummy);
  // repeats 10 times
  dummy = +Math.sqrt(+dummy);
  dummy = +Math.sqrt(+dummy);

  dummy = dummy|0;

  dummy = Math.imul(dummy|0, factor|0)|0;
  dummy = Math.imul(dummy|0, factor|0)|0;
  // repeats 20 times
  dummy = Math.imul(dummy|0, factor|0)|0;
  dummy = Math.imul(dummy|0, factor|0)|0;

  let end_time = Atomics.load(timer, 0)|0;

  return {
    dummy: dummy|0,
    time: ((end_time|0) != (start_time|0))|0
  };
}
\end{lstlisting}

The \texttt{squip} function is run repeatedly, summing up the return values of each iteration.
At the end of the timeslice, this sum is compared to a threshold.
If it is above the threshold, a \texttt{1} is registered, else a \texttt{0}.

\section{Optimization}
JavaSQUIP is intended to show SQUIP's potential as an attack in a browser setting.
Because of this, we aimed to make it as fast and stable as possible.

\subsection{Delay threshold}
The simplest parameter in SQUIP is the delay threshold.
It determines whether the summed up delay of a timeslice is registered as a \texttt{0} or a \texttt{1}.
If the threshold is too high or too low, results are skewed towards registering a \texttt{0} or a \texttt{1} respectively.

To tune this parameter, the resulting bitstream was checked against the actual sent stream.
All of the bits that were wrongly registered as \texttt{1} or wrongly \texttt{0} were tallied up separately.
Then the delay threshold was adjusted until fault 1's and faulty 0's were approximately equally frequent,
meaning that no skew toward high or low bits is present and the delay threshold is set optimally.

\subsection{Number of instructions for a queue stall}
The sender blocks a targeted scheduler queue by sending a large number of instructions to it.
The number of instructions is another tunable parameter:
it needs to be big enough to cause a queue stall and a spike in execution time,
yet it needs to be as small as possible to minimise chance of a stall bleeding into the following timeslice.

To tune the number of \texttt{imul} instructions, a bleed-over test was constructed.
It test consists of a string of alternating \texttt{1}s and \texttt{0}s, so contrasts between different bits could be easily observed.
The amount of \texttt{imul} instructions was tuned by successively reducing the number and running a test.
As soon as no spike in execution time was registered (see \textbf{image todo}), the lower limit was found.

\subsection{Minimising contention by the receivers}
The receiver detects a queue stall by measuring the time delay for an \texttt{imul} instruction.
To get a more stable result, several successive \texttt{imul} instructions were used.
The goal here was to get reliable results, while minimising the contention caused by the receivers themselves.
This is helpful in the transmission, as it reduces the contention for two co-located receivers,
maximizing the contrast to the receiver that is co-located with the sender (see \ref{sec:co-location})

The detection test was constructed similarly to the bleed-over test:
alternating \texttt{1}s and \texttt{0}s were received, and the delays measured.
The number of \texttt{imul} instructions in the receiver worker was reduced until no signal was retreivable,
resulting in a lower bound for the number of \texttt{imul} instructions in the receiver worker.

In addition to tuning the number of \texttt{imul} instructions, two more optimizations were done in the receiver worker:
buffer instructions and a delay loop. \\
In the measurement function, 10 \texttt{sqrt} instructions serve as a buffer to delay the \texttt{imul} instrucions:
they delay the execution, while not creating contention on ALU1.
The implementation can be seen in \ref{sec:detect-sqc}.\\
A delay loop was added after each measurement to give the scheduler queue time to recover from the interference of the measurement.
Since no normal delay function gives a better accuracy than 1ms, a busy-waiting loop was used.
The following code shows the delay loop that was executed after each measurement.
\begin{lstlisting}[language=JavaScript]
let k = 0|0;
while(k < 1000|0) {
  dummy  ^= k|0;
  k += 1;
}
\end{lstlisting}


%--- EVALUATING JAVASQUIP --------------------------------------------------------------
\chapter{Evaluating JavaSQUIP}
\label{ch:evaluation}

To evaluate and measure different approaches, we set up a test scenario for JavaSQUIP.
It consists of two web pages communicating with each other solely over JavaSQUIP.

\section {Test setup}
\label{sec:testsetup}
The test setup is comprised of two webpages: a sender and a receiver.
The sender simulates the target, and the receiver simulates a page that is controlled by the attacker.
Because the attacker has little or no control over the target, we kept the sender page minimal.
The sender has only one working thread.
The working thread is sending either a 0 (no action) or a 1 (contention with codependent \texttt{Math.imul()} calls) for each timeslice.
With more control over the receiver, the attacker can ensure co-location from this side.
For our tests on \textbf{TODOCPU1} (Zen2) and \textbf{TODOCPU2} (Zen3), we needed 14 receiving workers.
We go into more detail on co-location in Chapter~\ref{sec:co-location}.

To distinguish a message from random noise on the CPU, the sender sends a preamble before the actual data.
The preamble is a sequence of the numbers from 100 through 109.
If the receiver sees two ascending bytes between 100 and 109, it is highly likely that this is part of the preamble --
a message was detected.
For easier evaluation of the transmission accuracy, the message is also stored on the receiver's side,
so that the number of "faulty 1" and "faulty 0" bits can be automatically calculated.
In our test setup, messages of 500 random bytes were sent at a transmission speed of 1000bits/s.
This results in a transmission time of $\frac{500 \times 8}{1000} = 4$ seconds.

\section{Speed}
The sender and receiver need to know the desired transmission speed beforehand to match up their timeslices.
This means that the transmission speed is a fixed value and needs no further calculation.
Due to reasons explained in Section~\ref{sec:accurate-timing}, our best achievable speed was 1000bits/s.

There is, however, still some overhead for the preamble (see Section~\ref{sec:testsetup}).
It is a fixed size of 10 bytes regardless of the message,
so the relative overhead tends to $0\%$ as the message size increases.

\section{Reliability}
There are two aspects that affect the reliability of the covert channel:
First, the sender being co-located with a receiver, and second, the resilience of the covert channel against noise.

To measure the actual co-location probability, we used the preamble system as described in Section~\ref{sec:testsetup}.
Threads are unlikely to be relocated during the 4 seconds of a transmission,
and the preamble is likely to be recognised if co-location is given (see noise resilience further down).
We can conclude that the rate of recognised preambles is a good measurement for the actual co-location probability.
Further, the resilience to noise can be measured by the accuracy of a message when the preamble was recognised.

In our tests, we found an average co-location probability of $88.7\%$ in a Zen 3 architecture and $96\%$ in Zen 4.
This is consistent with the theoretical $93.3\%$ calculated in Section~\ref{sec:co-location}.
When tested under stress with \texttt{stress --cpu 1}, we achieved an average co-location probability of $53.7\%$.

For measuring noise resistance, we only looked at those transmissions where the preamble was recognised.
This resulted in an average transmission accuracy of $99.2\%$ on Zen 3, and $99.3\%$ on Zen 4.
With \texttt{stress --cpu 1}, we observed an accuracy of $74\%$.

% TODO graphs

%--- CONCLUSION ----------------------------------------------------------------

\chapter{Conclusion}
\label{ch:conclusion}
In this paper, we ported the SQUIP~\cite{squip} attack to a JavaScript context, thus creating the JavaSQUIP covert channel.
JavaSQUIP has achieved a transmission rate of 1000 bits/s, proving an advantage in performance over attacking single-scheduler based architectures~\cite{Rokicki2022webport}.
Additionally, we proved a co-location rate of $88.7\%$/$96\%$ with a transmission accuracy of $99.2\%$/$99.3\%$ (Zen 3/Zen 4).
This shows that architectures with multiple scheduler queues present a bigger attack surface than their single-scheduler counterparts.

JavaSQUIP breaks the sandboxing model of a web browser context.
This means that one browser instance can communicate with another without using the network, thus being virtually undetectable for an unknowing victim.
This could potentially be used by malicious code within a website -- no matter its origin -- to exfiltrate data to another browser instance on the same computer.
Its speed and accuracy enable an attacker to transmit large amounts of data, such as images or short audio clips, within the lifetime of a malicious web page.

Though security updates in modern browsers slow down and hinder timing attacks, microarchitectural attacks have not yet been eliminated.
JavaSQUIP mostly uses JavaScript features that have already been restricted to improve security as much as feasible;
mitigating microarchitectural attacks in a browser setting proves to be increasingly hard as CPUs grow more and more complex.


%--- BIBLIOGRAPHY --------------------------------------------------------------

\printbibliography

\end{document}
